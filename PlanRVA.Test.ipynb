{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tazs and 2021 Block groups\n",
    "blockgroups = gpd.read_file(r'C:\\Users\\egreenwell\\OneDrive - PlanRVA\\Documents\\GitHub\\BG2021.shp')\n",
    "tazs = gpd.read_file(r'C:\\Users\\egreenwell\\OneDrive - PlanRVA\\Documents\\GitHub\\TAZ.shp')\n",
    "\n",
    "# Ensure both GeoDataFrames use the same CRS\n",
    "blockgroups = blockgroups.to_crs(tazs.crs)\n",
    "\n",
    "# Generate centroid points for block groups\n",
    "blockgroups['centroid'] = blockgroups.geometry.centroid\n",
    "\n",
    "# Create a new GeoDataFrame for the centroids\n",
    "bgpoints = gpd.GeoDataFrame(blockgroups.drop(columns='geometry'), geometry='centroid', crs=blockgroups.crs)\n",
    "\n",
    "# Perform the spatial join\n",
    "join_df = gpd.sjoin(bgpoints, tazs, how=\"right\", predicate=\"within\")\n",
    "\n",
    "# Drop geometry columns if they exist (for some reason this had to happen for it to reconcile the difference)\n",
    "if 'geometry' in join_df.columns:\n",
    "    join_df = join_df.drop(columns='geometry')\n",
    "if 'centroid' in join_df.columns:\n",
    "    join_df = join_df.drop(columns='centroid')\n",
    "\n",
    "# Save the DataFrame without geometry columns to an Excel file\n",
    "join_df.to_csv(r'C:\\Users\\egreenwell\\OneDrive - PlanRVA\\Documents\\GitHub\\join_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up definitions given\n",
    "def sort_share_matrix(matrix):\n",
    "    '''\n",
    "    Sorts a share matrix first so that columns with ones higher up are further to the left\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix (pandas.DataFrame):\n",
    "        The share matrix to be sorted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_matrix (pandas.DataFrame):\n",
    "        Sorted share matrix\n",
    "    '''\n",
    "    first_one = pd.Series(index = matrix.columns) #Define series to identify the index of the first one in each column\n",
    "    for col in matrix.columns:\n",
    "        try:\n",
    "            first_one[col] = list(matrix[col]).index(1) #Identify the index of the first one in the column\n",
    "        except ValueError:\n",
    "            continue #If there aren't any ones in the column, leave the value in `first_one` null\n",
    "\n",
    "    order = first_one.sort_values().index #Find order of columns so that columns with ones higher up are further to the left\n",
    "    return matrix[order]\n",
    "\n",
    "def classify_nesting(nesting):\n",
    "    '''\n",
    "    Classifies a nesting as one to one, one TAZ to many Block Groups, one Block Group to Many TAZs, or many to many\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nesting (length-2 tuple):\n",
    "        Length-2 tuple where the first element is the number of TAZs and the second is the number of block groups\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    classification (str):\n",
    "        Classification name\n",
    "    '''\n",
    "    if nesting[0] == 1 and nesting[1] == 1:\n",
    "        return 'One TAZ = One Block Group'\n",
    "    else:\n",
    "        if nesting[0] == 1:\n",
    "            return 'One TAZ = Many Block Groups'\n",
    "        elif nesting[1] == 1:\n",
    "            return 'Many TAZs = One Block Group'\n",
    "        else:\n",
    "            return 'Many TAZs = Many Block Groups'\n",
    "\n",
    "def classify_pop(population):\n",
    "    '''\n",
    "    Classifies a geography based on population into three categories: less than 600, between 600 and 3000 (bounds included), and more than 3000\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    population (numeric):\n",
    "        Population value to classify\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class (str):\n",
    "        Population classification\n",
    "    '''\n",
    "    if population < 600:\n",
    "        return '<600'\n",
    "    elif population <= 3000:\n",
    "        return '600-3000'\n",
    "    else:\n",
    "        return '>3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Input File\n"
     ]
    }
   ],
   "source": [
    "#ensure files are read correctly\n",
    "\n",
    "# Define the base file path\n",
    "base_path = r'C:\\Users\\egreenwell\\OneDrive - PlanRVA\\Documents\\GitHub'\n",
    "\n",
    "# Get the directory for output files\n",
    "start_time = datetime.now()\n",
    "output_path = os.path.join(base_path, 'TAZ_BG_Nestings_Output_' + start_time.strftime('%y%m%d%H%M%S'))\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define the input file path and ensure it is correct\n",
    "infile = os.path.join(base_path, 'join_df.csv')\n",
    "taz2bg_file = os.path.join(output_path, 'taz2bg.csv')\n",
    "bg2taz_file = os.path.join(output_path, 'bg2taz.csv')\n",
    "\n",
    "# Ensure the input file exists\n",
    "if not os.path.isfile(infile):\n",
    "    raise FileNotFoundError(f\"The input file {infile} does not exist. Please check the file path.\")\n",
    "\n",
    "print('Reading Input File')\n",
    "data = pd.read_csv(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Population and Share Matrices\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and drop rows with missing TAZ, GEOID, or TOTPOP values\n",
    "data = data.dropna(subset=['TAZ', 'GEOID', 'TOTPOP'])\n",
    "\n",
    "\n",
    "# Generate lists of unique TAZs and block groups\n",
    "tazs = data['TAZ'].value_counts().sort_index().index  # List of TAZs\n",
    "bgs = data['GEOID'].value_counts().sort_index().index  # List of block groups\n",
    "\n",
    "n_t = len(tazs)  # Number of TAZs\n",
    "n_b = len(bgs)  # Number of block groups\n",
    "\n",
    "print('Creating Population and Share Matrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Zero-Population TAZs and Block Groups\n"
     ]
    }
   ],
   "source": [
    "# The population matrix has the TAZs as the rows and the block groups as the columns.\n",
    "# Each entry is the population in the TAZ and the block group\n",
    "pop_matrix = pd.DataFrame(np.zeros((n_t, n_b)), index=tazs, columns=bgs)\n",
    "for row in data.index:\n",
    "    taz = data.loc[row, 'TAZ']\n",
    "    bg = data.loc[row, 'GEOID']\n",
    "    pop = data.loc[row, 'TOTPOP']\n",
    "    pop_matrix.loc[taz, bg] += pop\n",
    "\n",
    "# The \"Share Matrix\" has the TAZs as the rows and the block groups as the columns\n",
    "# It is equal to 1 if the TAZ and the Block Group share population, and 0 if they do not, indicating if a TAZ and block group share population\n",
    "share_matrix = pop_matrix.astype(bool).astype(int)\n",
    "share_matrix_backup = share_matrix.copy()\n",
    "\n",
    "# The row and column sums of the population matrix are the TAZ and block group populations, respectively\n",
    "taz_pops = pop_matrix.sum(1).astype(int)\n",
    "bg_pops = pop_matrix.sum(0).astype(int)\n",
    "\n",
    "print('Removing Zero-Population TAZs and Block Groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Share Matrix\n"
     ]
    }
   ],
   "source": [
    "#Count the number of block groups sharing population with each TAZ and TAZs sharing population with block groups\n",
    "bgs_in_tazs = share_matrix.sum(1)\n",
    "tazs_in_bgs = share_matrix.sum(0)\n",
    "\n",
    "#Identify TAZs and block groups with and without population\n",
    "zero_pop_tazs = bgs_in_tazs[bgs_in_tazs == 0].index\n",
    "pop_tazs = bgs_in_tazs[bgs_in_tazs > 0].index\n",
    "zero_pop_bgs = tazs_in_bgs[tazs_in_bgs == 0].index\n",
    "pop_bgs = tazs_in_bgs[tazs_in_bgs > 0].index\n",
    "\n",
    "#Remove zero-population TAZs and block groups from the share matrix (they will be added later)\n",
    "share_matrix = share_matrix.loc[pop_tazs, pop_bgs]\n",
    "\n",
    "print('Sorting Share Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2421278155.py:16: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  first_one = pd.Series(index = matrix.columns) #Define series to identify the index of the first one in each column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying TAZ/Block Group Nestings\n"
     ]
    }
   ],
   "source": [
    "sort_iter = 10 #Number of sorting iterations. Multiple are needed for correct sorting\n",
    "for i in range(2*sort_iter): #In each iteration sort by block groups, than by TAZs\n",
    "    share_matrix = sort_share_matrix(share_matrix)\n",
    "    share_matrix = share_matrix.T\n",
    "\n",
    "print('Identifying TAZ/Block Group Nestings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying TAZ/Block Group Nestings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\4254457838.py:14: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  taz2nesting = pd.Series(index = tazs)\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\4254457838.py:15: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  bg2nesting = pd.Series(index = bgs)\n"
     ]
    }
   ],
   "source": [
    "bgs = list(share_matrix.columns)\n",
    "tazs = list(share_matrix.index)\n",
    "\n",
    "n_bgs = len(bgs)\n",
    "n_tazs = len(tazs)\n",
    "\n",
    "bg_sums = share_matrix.sum(0)\n",
    "taz_sums = share_matrix.sum(1)\n",
    "\n",
    "nesting_id = 1\n",
    "taz_index = 0\n",
    "bg_index = 0\n",
    "\n",
    "taz2nesting = pd.Series(index = tazs)\n",
    "bg2nesting = pd.Series(index = bgs)\n",
    "nestings = pd.DataFrame(columns = ['n_tazs', 'n_bgs'])\n",
    "\n",
    "while taz_index < n_tazs or bg_index < min_bgs:\n",
    "\n",
    "    current_taz = tazs[taz_index]\n",
    "    current_bg = bgs[bg_index]\n",
    "\n",
    "    #Identify the minimum number of block groups and TAZs needed in the nesting as the row and column sums of the share matrix for the current TAZ and block group\n",
    "    min_bgs = share_matrix.loc[current_taz].sum()\n",
    "    min_tazs = share_matrix[current_bg].sum()\n",
    "\n",
    "    complete_nesting = False\n",
    "\n",
    "    while not complete_nesting:\n",
    "\n",
    "        more_tazs_needed = False\n",
    "        more_bgs_needed = False\n",
    "\n",
    "        #Check if more block groups need to be added to the nesting\n",
    "        for taz in tazs[taz_index:(taz_index + min_tazs)]: #Check all of the current TAZs in the nesting\n",
    "            test_row = share_matrix.loc[taz].copy() #Copy the row of the share matrix for testing\n",
    "            test_row[bgs[bg_index:(bg_index + min_bgs)]] = 0 #Set the test row values to zero for the block groups already in the nesting\n",
    "            if test_row.sum() > 0: #Check if there are additional block groups to be added, and if there are, add them\n",
    "                more_bgs_needed = True\n",
    "                min_bgs += test_row.sum()\n",
    "                break\n",
    "\n",
    "        #Now perform the same check, only add TAZs as necessary\n",
    "        for bg in bgs[bg_index:(bg_index + min_bgs)]:\n",
    "            test_col = share_matrix[bg].copy()\n",
    "            test_col[tazs[taz_index:(taz_index + min_tazs)]] = 0\n",
    "            if test_col.sum() > 0:\n",
    "                more_tazs_needed = True\n",
    "                min_tazs += test_col.sum()\n",
    "                break\n",
    "\n",
    "        #Now check if no TAZs or block groups were added. If they were, go back to the start of the while loop. If not, record the nesting.\n",
    "        if not more_tazs_needed and not more_bgs_needed:\n",
    "            complete_nesting = True\n",
    "\n",
    "            taz2nesting[tazs[taz_index:(taz_index + min_tazs)]] = nesting_id\n",
    "            bg2nesting[bgs[bg_index:(bg_index + min_bgs)]] = nesting_id\n",
    "\n",
    "            nestings.loc[nesting_id] = [min_tazs, min_bgs]\n",
    "\n",
    "            nesting_id += 1\n",
    "\n",
    "            taz_index += min_tazs\n",
    "            bg_index += min_bgs\n",
    "\n",
    "print('Classifying TAZ/Block Group Nestings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Output Files\n",
      "Go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
      "C:\\Users\\egreenwell\\AppData\\Local\\Temp\\ipykernel_59008\\2859427977.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bg2nesting['TAZ_%s'%(i+1)] = np.nan\n"
     ]
    }
   ],
   "source": [
    "nestings['nesting'] = list(zip(nestings['n_tazs'], nestings['n_bgs'])) #Create tuples with # of TAZs and # of block groups\n",
    "nestings['type'] = nestings['nesting'].apply(classify_nesting)\n",
    "\n",
    "max_tazs = nestings['n_tazs'].max()\n",
    "max_bgs = nestings['n_bgs'].max()\n",
    "\n",
    "taz2nesting = pd.DataFrame(taz2nesting)\n",
    "taz2nesting['nesting_id'] = taz2nesting[0]\n",
    "del taz2nesting[0]\n",
    "taz2nesting.index.name = 'taz_id'\n",
    "\n",
    "taz2nesting = taz2nesting.reset_index()\n",
    "taz2nesting['pop'] = taz2nesting['taz_id'].map(taz_pops)\n",
    "taz2nesting['nest_type'] = taz2nesting['nesting_id'].map(nestings['type'])\n",
    "taz2nesting['pop_class'] = taz2nesting['pop'].apply(classify_pop)\n",
    "taz2nesting = taz2nesting.set_index('taz_id')\n",
    "\n",
    "for taz in zero_pop_tazs:\n",
    "    taz2nesting.loc[taz] = [np.nan, 0, 'No Population', '0']\n",
    "\n",
    "for i in range(max_bgs):\n",
    "    taz2nesting['blkgrp_%s'%(i+1)] = np.nan\n",
    "\n",
    "for taz in taz2nesting.index:\n",
    "    taz_nesting = taz2nesting.loc[taz, 'nesting_id']\n",
    "    bgs_in_taz = list(bg2nesting[bg2nesting == taz_nesting].index)\n",
    "    bgs_in_taz += (max_bgs - len(bgs_in_taz))*[np.nan]\n",
    "    taz2nesting.loc[taz, ['blkgrp_%s'%(i+1) for i in range(max_bgs)]] = bgs_in_taz\n",
    "\n",
    "bg2nesting = pd.DataFrame(bg2nesting)\n",
    "bg2nesting['nesting_id'] = bg2nesting[0]\n",
    "del bg2nesting[0]\n",
    "bg2nesting.index.name = 'blkgrp_id'\n",
    "\n",
    "bg2nesting = bg2nesting.reset_index()\n",
    "bg2nesting['pop'] = bg2nesting['blkgrp_id'].map(bg_pops)\n",
    "bg2nesting['nest_type'] = bg2nesting['nesting_id'].map(nestings['type'])\n",
    "bg2nesting['pop_class'] = bg2nesting['pop'].apply(classify_pop)\n",
    "bg2nesting = bg2nesting.set_index('blkgrp_id')\n",
    "\n",
    "for bg in zero_pop_bgs:\n",
    "    bg2nesting.loc[bg] = [np.nan, 0, 'No Population', '0']\n",
    "\n",
    "for i in range(max_tazs):\n",
    "    bg2nesting['TAZ_%s'%(i+1)] = np.nan\n",
    "\n",
    "for bg in bg2nesting.index:\n",
    "    bg_nesting = bg2nesting.loc[bg, 'nesting_id']\n",
    "    tazs_in_bg = list(taz2nesting[taz2nesting['nesting_id'] == bg_nesting].index)\n",
    "    tazs_in_bg += (max_tazs - len(tazs_in_bg))*[np.nan]\n",
    "    bg2nesting.loc[bg, ['TAZ_%s'%(i+1) for i in range(max_tazs)]] = tazs_in_bg\n",
    "\n",
    "del taz2nesting['nesting_id']\n",
    "del bg2nesting['nesting_id']\n",
    "\n",
    "print('Writing Output Files')\n",
    "taz2nesting.sort_index().to_csv(taz2bg_file)\n",
    "bg2nesting.sort_index().to_csv(bg2taz_file)\n",
    "\n",
    "print('Go')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
